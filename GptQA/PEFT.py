# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_PEFT.ipynb.

# %% auto 0
__all__ = ['params', 'data', 'CastOutputToFloat', 'freeze_model', 'print_trainable_parameters']

# %% ../nbs/02_PEFT.ipynb 5
import os
from reinautils import *

# %% ../nbs/02_PEFT.ipynb 6
params=Parameters().from_json ('/home/notebooks/chat/GptQA/tokens.json').from_json ('/home/notebooks/chat/GptQA/config.json')

# %% ../nbs/02_PEFT.ipynb 7
os.environ["CUDA_VISIBLE_DEVICES"]="0"
os.environ["WANDB_DIR"]=params.path.wandb

# %% ../nbs/02_PEFT.ipynb 8
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model 
import transformers
from datasets import load_dataset
from peft import PeftModel, PeftConfig

# %% ../nbs/02_PEFT.ipynb 13
class CastOutputToFloat(nn.Sequential):
    """
    Custom module to cast the output of a forward pass to float32.
    """
    def forward(self, x): 
        return super().forward(x).to(torch.float32)

def freeze_model(model):
    '''
    Freeze model parameters for future adapter training
    '''
    for param in model.parameters():
        param.requires_grad = False

        # Cast smaller parameters (like layernorm) to fp32 for numerical stability
        if param.ndim == 1:
            param.data = param.data.to(torch.float16)

    # Enable gradient checkpointing to reduce the number of stored activations
    model.gradient_checkpointing_enable()

    # Enable gradients for model inputs
    model.enable_input_require_grads()


    # Replace the output embedding layer with a version that casts output to float32
    model.embed_out = CastOutputToFloat(model.embed_out)
    return (model)

# model.lm_head = CastOutputToFloat(model.lm_head)

# %% ../nbs/02_PEFT.ipynb 16
def print_trainable_parameters(model: nn.Module) -> None:
    """
    Prints the number of trainable parameters in the model.

    Args:
        model (nn.Module): PyTorch model whose parameters need to be counted.

    Returns:
        None
    """
    total_params = 0
    trainable_params = 0

    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count

        # Check if the parameter is trainable
        if param.requires_grad:
            trainable_params += param_count

    print(f"\nTotal trainable parameters: {trainable_params}")
    print(f"Total parameters: {total_params}")
    print(f"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%")


# %% ../nbs/02_PEFT.ipynb 19
data = load_dataset("0-hero/OIG-small-chip2")


# %% ../nbs/02_PEFT.ipynb 25
model.push_to_hub("yuval6967/RedPajama-INCITE-Chat-3B-lora",
                  use_auth_token=params.tokens.huggingface.notebooks,
                  commit_message="basic training",
                  private=True)
