{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from GptQA.crowler import *\n",
    "from GptQA.KnowledgeEmbedding import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GptQA\n",
    "\n",
    "> A Q/A framework using Open.AI tools - This is an educational Repo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install GptQA\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://yuval6957.github.io/reinautils/\n",
      "https://yuval6957.github.io/./torchutils.html\n",
      "HTTP Error 404: Not Found\n",
      "https://yuval6957.github.io/./index.html\n",
      "HTTP Error 404: Not Found\n",
      "https://yuval6957.github.io/./parameters.html\n",
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "crawl(\"https://yuval6957.github.io/reinautils/\",\"/home/hd/GptQA\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a file with all the text files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval : false\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "import pickle\n",
    "texts=text2data(\"/home/hd/GptQA/text\",'txt',recursive=True)\n",
    "\n",
    "print (texts[:5]) \n",
    "\n",
    "with open(\"/home/hd/GptQA/text_accum.pkl\",\"wb\") as f:\n",
    "    pickle.dump(texts,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Embedding all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Set, Union, Callable\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenized = tokenize_data(texts, tokenizer, max_tokens = 500)\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to('cuda')\n",
    "\n",
    "embedded = embed_data(tokenized, partial(run_embeddings,model=model))\n",
    "\n",
    "with open(\"/home/hd/GptQA/embedding_all-mpnet-base-v2.pkl\",\"wb\") as f:\n",
    "    pickle.dump(embedded,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking a quation and getting the most relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What are the  language model I can use?'\n",
    "# question = 'How do I get access to GPT4'\n",
    "answers = top_scores(question, embedded,model,tokenizer)\n",
    "print(answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking a question ang getting an answer (using relevant models from Huggingface)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval : false\n",
    "# Loading the models for context creation\n",
    "context_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "context_model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to('cuda')\n",
    "\n",
    "# Loading the models for QA\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\", torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking a question and getting the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval : false\n",
    "question=\"What is our newest embeddings model?\"\n",
    "answer = answer_question(question, embedded, context_model=context_model, context_tokenizer = context_tokenizer,\n",
    "                         model = model, tokenizer = tokenizer, max_len = 1800, \n",
    "                    max_added_tokens = 150, \n",
    "                    temperature = 0.7,\n",
    "                    debug = False) \n",
    "print (answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
